{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a name=\"top\"></a>\n# **HW15 Meta Learning: Few-shot Classification**\n\nThis is the sample code for homework 15.\n\nPlease mail to mlta-2022-spring@googlegroups.com if you have any questions.","metadata":{"id":"wzVBe3h7Xh-2"}},{"cell_type":"markdown","source":"## **Step 0: Check GPU**","metadata":{"id":"RdpzIMG6XsGK"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"zjjHsZbaL7SV","outputId":"682a5363-9700-4fc7-c313-8f846a5a3278","execution":{"iopub.status.busy":"2022-06-27T14:14:43.293488Z","iopub.execute_input":"2022-06-27T14:14:43.293877Z","iopub.status.idle":"2022-06-27T14:14:44.056038Z","shell.execute_reply.started":"2022-06-27T14:14:43.293848Z","shell.execute_reply":"2022-06-27T14:14:44.054809Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"## **Step 1: Download Data**\n\nRun the cell to download data, which has been pre-processed by TAs.  \nThe dataset has been augmented, so extra data augmentation is not required.\n","metadata":{"id":"bQ3wvyjnXwGX"}},{"cell_type":"code","source":"workspace_dir = '.'\n\n# Download dataset\n!wget https://github.com/xraychen/shiny-disco/releases/download/Latest/omniglot.tar.gz \\\n    -O \"{workspace_dir}/Omniglot.tar.gz\"\n!wget https://github.com/xraychen/shiny-disco/releases/download/Latest/omniglot-test.tar.gz \\\n    -O \"{workspace_dir}/Omniglot-test.tar.gz\"\n\n# Use `tar' command to decompress\n!tar -zxf \"{workspace_dir}/Omniglot.tar.gz\" -C \"{workspace_dir}/\"\n!tar -zxf \"{workspace_dir}/Omniglot-test.tar.gz\" -C \"{workspace_dir}/\"","metadata":{"id":"g7Gt4Jucug41","outputId":"6b661097-2651-47a6-9431-25034630bb23","execution":{"iopub.status.busy":"2022-06-27T14:14:44.059114Z","iopub.execute_input":"2022-06-27T14:14:44.060356Z","iopub.status.idle":"2022-06-27T14:14:51.364633Z","shell.execute_reply.started":"2022-06-27T14:14:44.060306Z","shell.execute_reply":"2022-06-27T14:14:51.363329Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"## **Step 2: Build the model**","metadata":{"id":"baVsWfcSYHVN"}},{"cell_type":"markdown","source":"### Library importation","metadata":{"id":"gqiOdDLgYOlQ"}},{"cell_type":"code","source":"# Import modules we need\nimport glob, random\nfrom collections import OrderedDict\n\nimport numpy as np\nfrom tqdm.auto import tqdm\n\nimport torch, torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\nfrom IPython.display import display\n\n# Check device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"DEVICE = {device}\")\n\n# Fix random seeds\nrandom_seed = 0\nrandom.seed(random_seed)\nnp.random.seed(random_seed)\ntorch.manual_seed(random_seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(random_seed)","metadata":{"id":"-9pfkqh8gxHD","outputId":"077d3bb8-588b-4c57-d020-ffb5cc955a40","execution":{"iopub.status.busy":"2022-06-27T14:14:51.366795Z","iopub.execute_input":"2022-06-27T14:14:51.367178Z","iopub.status.idle":"2022-06-27T14:14:51.378158Z","shell.execute_reply.started":"2022-06-27T14:14:51.367139Z","shell.execute_reply":"2022-06-27T14:14:51.377216Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"### Model Construction Preliminaries\n\nSince our task is image classification, we need to build a CNN-based model.  \nHowever, to implement MAML algorithm, we should adjust some code in `nn.Module`.\n","metadata":{"id":"3TlwLtC1YRT7"}},{"cell_type":"markdown","source":"Take a look at MAML pseudocode...\n\n<img src=\"https://i.imgur.com/9aHlvfX.png\" width=\"50%\" />\n\nOn the 10-th line, what we take gradients on are those $\\theta$ representing  \n<font color=\"#0CC\">**the original model parameters**</font> (outer loop) instead of those in  the  \n<font color=\"#0C0\">**inner loop**</font>, so we need to use `functional_forward` to compute the output  \nlogits of input image instead of `forward` in `nn.Module`.\n\nThe following defines these functions.\n\n<!-- 由於在第10行，我們是要對原本的參數 θ 微分，並非 inner-loop (Line5~8) 的 θ' 微分，因此在 inner-loop，我們需要用 functional forward 的方式算出 input image 的 output logits，而不是直接用 nn.module 裡面的 forward（直接對 θ 微分）。在下面我們分別定義了 functional forward 以及 forward 函數。 -->","metadata":{"id":"dFwB3tuEDYfy"}},{"cell_type":"markdown","source":"### Model block definition","metadata":{"id":"iuYQiPeQYc__"}},{"cell_type":"code","source":"def ConvBlock(in_ch: int, out_ch: int):\n    return nn.Sequential(\n        nn.Conv2d(in_ch, out_ch, 3, padding=1),\n        nn.BatchNorm2d(out_ch),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=2),\n    )\n\n\ndef ConvBlockFunction(x, w, b, w_bn, b_bn):\n    x = F.conv2d(x, w, b, padding=1)\n    x = F.batch_norm(\n        x, running_mean=None, running_var=None, weight=w_bn, bias=b_bn, training=True\n    )\n    x = F.relu(x)\n    x = F.max_pool2d(x, kernel_size=2, stride=2)\n    return x","metadata":{"id":"GgFbbKHYg3Hk","execution":{"iopub.status.busy":"2022-06-27T14:14:51.381139Z","iopub.execute_input":"2022-06-27T14:14:51.382383Z","iopub.status.idle":"2022-06-27T14:14:51.391161Z","shell.execute_reply.started":"2022-06-27T14:14:51.382346Z","shell.execute_reply":"2022-06-27T14:14:51.390243Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"### Model definition","metadata":{"id":"iQEzgWN7fi7B"}},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, in_ch, k_way):\n        super(Classifier, self).__init__()\n        self.conv1 = ConvBlock(in_ch, 64)\n        self.conv2 = ConvBlock(64, 64)\n        self.conv3 = ConvBlock(64, 64)\n        self.conv4 = ConvBlock(64, 64)\n        self.logits = nn.Linear(64, k_way)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = x.view(x.shape[0], -1)\n        x = self.logits(x)\n        return x\n\n    def functional_forward(self, x, params):\n        \"\"\"\n        Arguments:\n        x: input images [batch, 1, 28, 28]\n        params: model parameters,\n                i.e. weights and biases of convolution\n                     and weights and biases of\n                                   batch normalization\n                type is an OrderedDict\n\n        Arguments:\n        x: input images [batch, 1, 28, 28]\n        params: The model parameters,\n                i.e. weights and biases of convolution\n                     and batch normalization layers\n                It's an `OrderedDict`\n        \"\"\"\n        for block in [1, 2, 3, 4]:\n            x = ConvBlockFunction(\n                x,\n                params[f\"conv{block}.0.weight\"],\n                params[f\"conv{block}.0.bias\"],\n                params.get(f\"conv{block}.1.weight\"),\n                params.get(f\"conv{block}.1.bias\"),\n            )\n        x = x.view(x.shape[0], -1)\n        x = F.linear(x, params[\"logits.weight\"], params[\"logits.bias\"])\n        return x","metadata":{"id":"0bFBGEQoHQUW","execution":{"iopub.status.busy":"2022-06-27T14:14:51.392292Z","iopub.execute_input":"2022-06-27T14:14:51.394866Z","iopub.status.idle":"2022-06-27T14:14:51.408347Z","shell.execute_reply.started":"2022-06-27T14:14:51.394830Z","shell.execute_reply":"2022-06-27T14:14:51.407249Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"### Create Label\n\nThis function is used to create labels.  \nIn a N-way K-shot few-shot classification problem,\neach task has `n_way` classes, while there are `k_shot` images for each class.  \nThis is a function that creates such labels.\n","metadata":{"id":"gmJq_0B9Yj0G"}},{"cell_type":"code","source":"def create_label(n_way, k_shot):\n    return torch.arange(n_way).repeat_interleave(k_shot).long()\n\n\n# Try to create labels for 5-way 2-shot setting\ncreate_label(5, 2)","metadata":{"id":"GQF5vgLvg5aX","outputId":"aa1c2ea2-1164-41d2-b3a0-14cf5fca1f0e","execution":{"iopub.status.busy":"2022-06-27T14:14:51.409669Z","iopub.execute_input":"2022-06-27T14:14:51.410389Z","iopub.status.idle":"2022-06-27T14:14:51.427969Z","shell.execute_reply.started":"2022-06-27T14:14:51.410353Z","shell.execute_reply":"2022-06-27T14:14:51.426584Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy calculation","metadata":{"id":"2nCFv9PGw50J"}},{"cell_type":"code","source":"def calculate_accuracy(logits, labels):\n    \"\"\"utility function for accuracy calculation\"\"\"\n    acc = np.asarray(\n        [(torch.argmax(logits, -1).cpu().numpy() == labels.cpu().numpy())]\n    ).mean()\n    return acc","metadata":{"id":"FahDr0xQw50S","execution":{"iopub.status.busy":"2022-06-27T14:14:51.429687Z","iopub.execute_input":"2022-06-27T14:14:51.430433Z","iopub.status.idle":"2022-06-27T14:14:51.437404Z","shell.execute_reply.started":"2022-06-27T14:14:51.430389Z","shell.execute_reply":"2022-06-27T14:14:51.436507Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"### Define Dataset\n\nDefine the dataset.  \nThe dataset returns images of a random character, with (`k_shot + q_query`) images,  \nso the size of returned tensor is `[k_shot+q_query, 1, 28, 28]`.  \n","metadata":{"id":"9Hl7ro2mYzsI"}},{"cell_type":"code","source":"# Dataset for train and val\nclass Omniglot(Dataset):\n    def __init__(self, data_dir, k_way, q_query, task_num=None):\n        self.file_list = [\n            f for f in glob.glob(data_dir + \"**/character*\", recursive=True)\n        ]\n        # limit task number if task_num is set\n        if task_num is not None:\n            self.file_list = self.file_list[: min(len(self.file_list), task_num)]\n        print(\"task_num = \", task_num)\n        self.transform = transforms.Compose([\n          # transforms.RandomHorizontalFlip(p=0.5),\n          # transforms.RandomRotation(20),\n          transforms.ToTensor()\n        ])\n        self.n = k_way + q_query\n\n    def __getitem__(self, idx):\n        sample = np.arange(20)\n\n        # For random sampling the characters we want.\n        np.random.shuffle(sample)\n        img_path = self.file_list[idx]\n        img_list = [f for f in glob.glob(img_path + \"**/*.png\", recursive=True)]\n        img_list.sort()\n        imgs = [self.transform(Image.open(img_file)) for img_file in img_list]\n        # `k_way + q_query` examples for each character\n        imgs = torch.stack(imgs)[sample[: self.n]]\n        return imgs\n\n    def __len__(self):\n        return len(self.file_list)","metadata":{"id":"-tJ2mot9hHPb","execution":{"iopub.status.busy":"2022-06-27T14:14:51.439093Z","iopub.execute_input":"2022-06-27T14:14:51.439837Z","iopub.status.idle":"2022-06-27T14:14:51.451434Z","shell.execute_reply.started":"2022-06-27T14:14:51.439800Z","shell.execute_reply":"2022-06-27T14:14:51.450479Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"## **Step 3: Learning Algorithms**\n\n### Transfer learning\n\nThe solver first chose five task from the training set, then do normal classification training on the chosen five tasks. In inference, the model finetune for `inner_train_step` steps on the support set images, and than do inference on the query set images.\n\nFor consistant with the meta-learning solver, the base solver has the exactly same input and output format with the meta-learning solver.\n\n","metadata":{"id":"WRzjBWhwI6tc"}},{"cell_type":"code","source":"def BaseSolver(\n    model,\n    optimizer,\n    x,\n    n_way,\n    k_shot,\n    q_query,\n    loss_fn,\n    inner_train_step=1,\n    inner_lr=0.4,\n    train=True,\n    return_labels=False,\n):\n    criterion, task_loss, task_acc = loss_fn, [], []\n    labels = []\n\n    for meta_batch in x:\n        # Get data\n        support_set = meta_batch[: n_way * k_shot]\n        query_set = meta_batch[n_way * k_shot :]\n\n        if train:\n            \"\"\" training loop \"\"\"\n            # Use the support set to calculate loss\n            labels = create_label(n_way, k_shot).to(device)\n            logits = model.forward(support_set)\n            loss = criterion(logits, labels)\n\n            task_loss.append(loss)\n            task_acc.append(calculate_accuracy(logits, labels))\n        else:\n            \"\"\" validation / testing loop \"\"\"\n            # First update model with support set images for `inner_train_step` steps\n            fast_weights = OrderedDict(model.named_parameters())\n\n\n            for inner_step in range(inner_train_step):\n                # Simply training\n                train_label = create_label(n_way, k_shot).to(device)\n                logits = model.functional_forward(support_set, fast_weights)\n                loss = criterion(logits, train_label)\n\n                grads = torch.autograd.grad(loss, fast_weights.values(), create_graph=True)\n                # Perform SGD\n                fast_weights = OrderedDict(\n                    (name, param - inner_lr * grad)\n                    for ((name, param), grad) in zip(fast_weights.items(), grads)\n                )\n\n            if not return_labels:\n                \"\"\" validation \"\"\"\n                val_label = create_label(n_way, q_query).to(device)\n\n                logits = model.functional_forward(query_set, fast_weights)\n                loss = criterion(logits, val_label)\n                task_loss.append(loss)\n                task_acc.append(calculate_accuracy(logits, val_label))\n            else:\n                \"\"\" testing \"\"\"\n                logits = model.functional_forward(query_set, fast_weights)\n                labels.extend(torch.argmax(logits, -1).cpu().numpy())\n\n    if return_labels:\n        return labels\n\n    batch_loss = torch.stack(task_loss).mean()\n    task_acc = np.mean(task_acc)\n\n    if train:\n        # Update model\n        model.train()\n        optimizer.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\n\n    return batch_loss, task_acc","metadata":{"id":"R_jGPJHK7KpC","execution":{"iopub.status.busy":"2022-06-27T14:14:51.454857Z","iopub.execute_input":"2022-06-27T14:14:51.455583Z","iopub.status.idle":"2022-06-27T14:14:51.471500Z","shell.execute_reply.started":"2022-06-27T14:14:51.455532Z","shell.execute_reply":"2022-06-27T14:14:51.470599Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"### Meta Learning\n\nHere is the main Meta Learning algorithm.\n\nPlease finish the TODO blocks for the inner and outer loop update rules.\n\n- For implementing FO-MAML you can refer to [p.25 of the slides](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta1%20(v6).pdf#page=25&view=FitW).\n\n- For the original MAML, you can refer to [the slides of meta learning (p.13 ~ p.18)](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta1%20(v6).pdf#page=13&view=FitW).\n","metadata":{"id":"Gm5iVp90Ylii"}},{"cell_type":"code","source":"def MetaSolver(\n    model,\n    optimizer,\n    x,\n    n_way,\n    k_shot,\n    q_query,\n    loss_fn,\n    inner_train_step=1,\n    inner_lr=0.4,\n    train=True,\n    return_labels=False\n):\n    criterion, task_loss, task_acc = loss_fn, [], []\n    labels = []\n\n    for meta_batch in x:\n        # Get data\n        support_set = meta_batch[: n_way * k_shot]\n        query_set = meta_batch[n_way * k_shot :]\n\n        # Copy the params for inner loop\n        fast_weights = OrderedDict(model.named_parameters())\n\n        ### ---------- INNER TRAIN LOOP ---------- ###\n        for inner_step in range(inner_train_step):\n            # Simply training\n            train_label = create_label(n_way, k_shot).to(device)\n            logits = model.functional_forward(support_set, fast_weights)\n            loss = criterion(logits, train_label)\n            # Inner gradients update! vvvvvvvvvvvvvvvvvvvv #\n            \"\"\" Inner Loop Update \"\"\"\n            # TODO: Finish the inner loop update rule\n            grads = torch.autograd.grad(loss, fast_weights.values(), create_graph=True)\n            fast_weights = OrderedDict(\n                (name, param - inner_lr * grad)\n                for ((name, param), grad)\n                    in zip(fast_weights.items(), grads))\n            # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ #\n\n        ### ---------- INNER VALID LOOP ---------- ###\n        if not return_labels:\n            \"\"\" training / validation \"\"\"\n            val_label = create_label(n_way, q_query).to(device)\n\n            # Collect gradients for outer loop\n            logits = model.functional_forward(query_set, fast_weights)\n            loss = criterion(logits, val_label)\n            task_loss.append(loss)\n            task_acc.append(calculate_accuracy(logits, val_label))\n        else:\n            \"\"\" testing \"\"\"\n            logits = model.functional_forward(query_set, fast_weights)\n            labels.extend(torch.argmax(logits, -1).cpu().numpy())\n\n    if return_labels:\n        return labels\n\n    # Update outer loop\n    model.train()\n    optimizer.zero_grad()\n\n    meta_batch_loss = torch.stack(task_loss).mean()\n    if train:\n        \"\"\" Outer Loop Update \"\"\"\n        # TODO: Finish the outer loop update\n        meta_batch_loss.backward()\n        optimizer.step()\n\n    task_acc = np.mean(task_acc)\n    return meta_batch_loss, task_acc","metadata":{"id":"KjNxrWW_yNck","execution":{"iopub.status.busy":"2022-06-27T14:14:51.475782Z","iopub.execute_input":"2022-06-27T14:14:51.476318Z","iopub.status.idle":"2022-06-27T14:14:51.489826Z","shell.execute_reply.started":"2022-06-27T14:14:51.476284Z","shell.execute_reply":"2022-06-27T14:14:51.488869Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"## **Step 4: Initialization**\n\nAfter defining all components we need, the following initialize a model before training.","metadata":{"id":"nBoRBhVlZAST"}},{"cell_type":"markdown","source":"### Hyperparameters ","metadata":{"id":"Ip-i7aseftUF"}},{"cell_type":"code","source":"n_way = 5\nk_shot = 1\nq_query = 1\ntrain_inner_train_step = 1\nval_inner_train_step = 3    #### 3 experiments and record the best val acc\ninner_lr = 0.7 # 0.4\nmeta_lr = 0.002 # 0.001\nmeta_batch_size = 32\nmax_epoch = 80 # 30\neval_batches = 20\ntrain_data_path = \"./Omniglot/images_background/\"","metadata":{"id":"0wFHmVcBhE4M","execution":{"iopub.status.busy":"2022-06-27T14:14:51.491459Z","iopub.execute_input":"2022-06-27T14:14:51.492490Z","iopub.status.idle":"2022-06-27T14:14:51.505719Z","shell.execute_reply.started":"2022-06-27T14:14:51.492445Z","shell.execute_reply":"2022-06-27T14:14:51.504580Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":"### Dataloader initialization","metadata":{"id":"Uvzo7NVpfu5V"}},{"cell_type":"code","source":"def dataloader_init(datasets, shuffle=True, num_workers=2):\n    train_set, val_set = datasets\n    train_loader = DataLoader(\n        train_set,\n        # The \"batch_size\" here is not \\\n        #    the meta batch size, but  \\\n        #    how many different        \\\n        #    characters in a task,     \\\n        #    i.e. the \"n_way\" in       \\\n        #    few-shot classification.\n        batch_size=n_way,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        drop_last=True,\n    )\n    val_loader = DataLoader(\n        val_set, batch_size=n_way, num_workers=num_workers, shuffle=shuffle, drop_last=True\n    )\n\n    train_iter = iter(train_loader)\n    val_iter = iter(val_loader)\n    return (train_loader, val_loader), (train_iter, val_iter)","metadata":{"id":"3I13GJavhP0_","execution":{"iopub.status.busy":"2022-06-27T14:14:51.507107Z","iopub.execute_input":"2022-06-27T14:14:51.507514Z","iopub.status.idle":"2022-06-27T14:14:51.521472Z","shell.execute_reply.started":"2022-06-27T14:14:51.507478Z","shell.execute_reply":"2022-06-27T14:14:51.520474Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"### Model & optimizer initialization","metadata":{"id":"KVund--bfw0e"}},{"cell_type":"code","source":"def model_init():\n    meta_model = Classifier(1, n_way).to(device)\n    optimizer = torch.optim.Adam(meta_model.parameters(), lr=meta_lr)\n    loss_fn = nn.CrossEntropyLoss().to(device)\n    return meta_model, optimizer, loss_fn","metadata":{"id":"Kxug882ihF2B","execution":{"iopub.status.busy":"2022-06-27T14:14:51.523021Z","iopub.execute_input":"2022-06-27T14:14:51.524081Z","iopub.status.idle":"2022-06-27T14:14:51.532505Z","shell.execute_reply.started":"2022-06-27T14:14:51.524044Z","shell.execute_reply":"2022-06-27T14:14:51.531435Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"### Utility function to get a meta-batch","metadata":{"id":"gj8cLRNLf2zg"}},{"cell_type":"code","source":"def get_meta_batch(meta_batch_size, k_shot, q_query, data_loader, iterator):\n    data = []\n    for _ in range(meta_batch_size):\n        try:\n            # a \"task_data\" tensor is representing \\\n            #     the data of a task, with size of \\\n            #     [n_way, k_shot+q_query, 1, 28, 28]\n            task_data = iterator.next()\n        except StopIteration:\n            iterator = iter(data_loader)\n            task_data = iterator.next()\n        train_data = task_data[:, :k_shot].reshape(-1, 1, 28, 28)\n        val_data = task_data[:, k_shot:].reshape(-1, 1, 28, 28)\n        task_data = torch.cat((train_data, val_data), 0)\n        data.append(task_data)\n    return torch.stack(data).to(device), iterator","metadata":{"id":"zrkCSsxOhC-N","execution":{"iopub.status.busy":"2022-06-27T14:14:51.533839Z","iopub.execute_input":"2022-06-27T14:14:51.534258Z","iopub.status.idle":"2022-06-27T14:14:51.545423Z","shell.execute_reply.started":"2022-06-27T14:14:51.534223Z","shell.execute_reply":"2022-06-27T14:14:51.544409Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"mainprog\" id=\"mainprog\"></a>\n## **Step 5: Main program for training & testing**","metadata":{"id":"pWQczA3FwjEG"}},{"cell_type":"markdown","source":"### Start training!\nWith `solver = 'base'`, the solver is a transfer learning algorithm.\n\nOnce you finish the TODO blocks in the `MetaSolver`, change the variable `solver = 'meta'` to start training with meta learning algorithm.\n","metadata":{"id":"8EirEnaof7ep"}},{"cell_type":"code","source":"solver = 'meta' # base, meta\nmeta_model, optimizer, loss_fn = model_init()\n\n# init solver and dataset according to solver type\nif solver == 'base':\n    max_epoch = 5 # the base solver only needs 5 epochs\n    Solver = BaseSolver\n    train_set, val_set = torch.utils.data.random_split(\n        Omniglot(train_data_path, k_shot, q_query, task_num=10), [5, 5]\n    )\n    (train_loader, val_loader), (train_iter, val_iter) = dataloader_init((train_set, val_set), shuffle=False)\n\nelif solver == 'meta':\n    Solver = MetaSolver\n    dataset = Omniglot(train_data_path, k_shot, q_query, task_num=40)    #### 3 experiments of task_num and record the best val_acc\n    train_split = int(0.8 * len(dataset))\n    val_split = len(dataset) - train_split\n    train_set, val_set = torch.utils.data.random_split(\n        dataset, [train_split, val_split]\n    )\n    (train_loader, val_loader), (train_iter, val_iter) = dataloader_init((train_set, val_set))\nelse:\n    raise NotImplementedError\n\n\n# main training loop\nfor epoch in range(max_epoch):\n    print(\"Epoch %d\" % (epoch + 1))\n    train_meta_loss = []\n    train_acc = []\n    # The \"step\" here is a meta-gradinet update step\n    for step in tqdm(range(max(1, len(train_loader) // meta_batch_size))):\n        x, train_iter = get_meta_batch(\n            meta_batch_size, k_shot, q_query, train_loader, train_iter\n        )\n        meta_loss, acc = Solver(\n            meta_model,\n            optimizer,\n            x,\n            n_way,\n            k_shot,\n            q_query,\n            loss_fn, \n            inner_train_step=train_inner_train_step\n        )\n        train_meta_loss.append(meta_loss.item())\n        train_acc.append(acc)\n    print(\"  Loss    : \", \"%.3f\" % (np.mean(train_meta_loss)), end=\"\\t\")\n    print(\"  Accuracy: \", \"%.3f %%\" % (np.mean(train_acc) * 100))\n\n    # See the validation accuracy after each epoch.\n    # Early stopping is welcomed to implement.\n    val_acc = []\n    for eval_step in tqdm(range(max(1, len(val_loader) // (eval_batches)))):\n        x, val_iter = get_meta_batch(\n            eval_batches, k_shot, q_query, val_loader, val_iter\n        )\n        # We update three inner steps when testing.\n        _, acc = Solver(\n            meta_model,\n            optimizer,\n            x,\n            n_way,\n            k_shot,\n            q_query,\n            loss_fn,\n            inner_train_step=val_inner_train_step,\n            train=False,\n        )\n        val_acc.append(acc)\n    print(\"  Validation accuracy: \", \"%.3f %%\" % (np.mean(val_acc) * 100))","metadata":{"id":"JQZjJrLAhBWw","outputId":"2d9f8c4c-b42d-4627-9e6d-f194e3b2e66f","execution":{"iopub.status.busy":"2022-06-27T14:14:51.547098Z","iopub.execute_input":"2022-06-27T14:14:51.547521Z","iopub.status.idle":"2022-06-27T14:21:45.133482Z","shell.execute_reply.started":"2022-06-27T14:14:51.547485Z","shell.execute_reply":"2022-06-27T14:21:45.131951Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"### Testing the result\n\nSince the testing data is sampled by TAs in advance, you should not change the code in `OmnigloTest` dataset, otherwise your score may not be correct on the Kaggle leaderboard.\n\nHowever, fell free to chagne the variable `inner_train_step` to set the training steps on the query set images.","metadata":{"id":"u5Ew8-POf9sw"}},{"cell_type":"code","source":"import os\n\n# test dataset\nclass OmniglotTest(Dataset):\n    def __init__(self, test_dir):\n        self.test_dir = test_dir\n        self.n = 5\n\n        self.transform = transforms.Compose([transforms.ToTensor()])\n\n    def __getitem__(self, idx):\n        support_files = [\n            os.path.join(self.test_dir, \"support\", f\"{idx:>04}\", f\"image_{i}.png\")\n            for i in range(self.n)\n        ]\n        query_files = [\n            os.path.join(self.test_dir, \"query\", f\"{idx:>04}\", f\"image_{i}.png\")\n            for i in range(self.n)\n        ]\n\n        support_imgs = torch.stack(\n            [self.transform(Image.open(e)) for e in support_files]\n        )\n        query_imgs = torch.stack([self.transform(Image.open(e)) for e in query_files])\n\n        return support_imgs, query_imgs\n\n    def __len__(self):\n        return len(os.listdir(os.path.join(self.test_dir, \"support\")))","metadata":{"id":"OKtTzxZeln5Z","execution":{"iopub.status.busy":"2022-06-27T14:21:45.136154Z","iopub.execute_input":"2022-06-27T14:21:45.137387Z","iopub.status.idle":"2022-06-27T14:21:45.150116Z","shell.execute_reply.started":"2022-06-27T14:21:45.137345Z","shell.execute_reply":"2022-06-27T14:21:45.149149Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"test_inner_train_step = 10 # you can change this\n\ntest_batches = 20\ntest_dataset = OmniglotTest(\"Omniglot-test\")\ntest_loader = DataLoader(test_dataset, batch_size=test_batches, shuffle=False)\n\noutput = []\nfor _, batch in enumerate(tqdm(test_loader)):\n    support_set, query_set = batch\n    x = torch.cat([support_set, query_set], dim=1)\n    x = x.to(device)\n\n    labels = Solver(\n        meta_model,\n        optimizer,\n        x,\n        n_way,\n        k_shot,\n        q_query,\n        loss_fn,\n        inner_train_step=test_inner_train_step,\n        train=False,\n        return_labels=True,\n    )\n\n    output.extend(labels)\n\n# write to csv\nwith open(\"output.csv\", \"w\") as f:\n    f.write(f\"id,class\\n\")\n    for i, label in enumerate(output):\n        f.write(f\"{i},{label}\\n\")","metadata":{"id":"kTWHs1RThgGc","outputId":"4b4d0671-843f-4698-b791-57880e875cb7","execution":{"iopub.status.busy":"2022-06-27T14:21:45.152246Z","iopub.execute_input":"2022-06-27T14:21:45.152764Z","iopub.status.idle":"2022-06-27T14:22:05.800712Z","shell.execute_reply.started":"2022-06-27T14:21:45.152725Z","shell.execute_reply":"2022-06-27T14:22:05.799711Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"Download the `output.csv` and submit to Kaggle!","metadata":{"id":"yIfamxgMIaXw"}},{"cell_type":"markdown","source":"### Plots for the report","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nval_inner_train_step=[1,2,3]\nbest_val_acc=[0.96,0.98,0.98]\nplt.xticks(val_inner_train_step)\nplt.xlabel(\"val_inner_train_step\")\nplt.ylabel(\"best_val_acc\")\nplt.title(\"Q2\")\nplt.plot(val_inner_train_step, best_val_acc)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T14:30:15.230345Z","iopub.execute_input":"2022-06-27T14:30:15.230720Z","iopub.status.idle":"2022-06-27T14:30:15.404237Z","shell.execute_reply.started":"2022-06-27T14:30:15.230688Z","shell.execute_reply":"2022-06-27T14:30:15.403321Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ntask_num=[35,40,45]\nbest_val_acc=[0.87,0.91,0.93]\nplt.xticks(task_num) \nplt.xlabel(\"task_num\")\nplt.ylabel(\"best_val_acc\")\nplt.title(\"Q1\")\nplt.plot(task_num, best_val_acc)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T14:29:58.083043Z","iopub.execute_input":"2022-06-27T14:29:58.083655Z","iopub.status.idle":"2022-06-27T14:29:58.237896Z","shell.execute_reply.started":"2022-06-27T14:29:58.083618Z","shell.execute_reply":"2022-06-27T14:29:58.236953Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"## **Reference**\n1. Chelsea Finn, Pieter Abbeel, & Sergey Levine. (2017). [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.](https://arxiv.org/abs/1909.09157)\n1. Aniruddh Raghu, Maithra Raghu, Samy Bengio, & Oriol Vinyals. (2020). [Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML.](https://arxiv.org/abs/1909.09157)","metadata":{"id":"rtD8X3RLf-6w"}}]}